{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7313e9-1b46-474e-b4ce-3f47037d0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import diffusers\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bcaac1-d582-4669-b009-4506fb871a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import transformers\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, set_seed\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from diffusers.models import UNet2DConditionModel, AutoencoderKL\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed32ea78-aec1-4b08-a834-cdf75045f724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe = diffusers.DiffusionPipeline.from_pretrained('data/model_bins/stable-diffusion-xl-base-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7678ff0e-d140-4a77-b1fc-65c472b79619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c12c259-8f30-453f-a9f0-60313594636c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>key</th>\n",
       "      <th>sha256</th>\n",
       "      <th>url</th>\n",
       "      <th>llava_caption</th>\n",
       "      <th>nsfw_prediction</th>\n",
       "      <th>alt_txt</th>\n",
       "      <th>alt_txt_similarity</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>original_width</th>\n",
       "      <th>original_height</th>\n",
       "      <th>exif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000623232</td>\n",
       "      <td>d46d20ff9fa7212b3671b49ae73371672711e5265fff29...</td>\n",
       "      <td>https://img1.etsystatic.com/000/0/5370909/il_f...</td>\n",
       "      <td>a black and white drawing of a man climbing a ...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>Art Deco Man Cave : Rockwell kent art deco man...</td>\n",
       "      <td>0.592961</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000616883</td>\n",
       "      <td>dde6e39a297da2d7e8923bdf569c0e4704b29a69148b9d...</td>\n",
       "      <td>https://www.aktay.net/wp-content/uploads/2014/...</td>\n",
       "      <td>a magazine or brochure that is open to a page ...</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>pergo broÅŸÃ¼r tasarÄ±mÄ± 12</td>\n",
       "      <td>0.644895</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000620777</td>\n",
       "      <td>7196431abc8b8fabfa34f50be738ba1c0a4c72ccbc70ef...</td>\n",
       "      <td>https://cdn5.vectorstock.com/i/1000x1000/03/14...</td>\n",
       "      <td>a gold and black building logo. The logo featu...</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>Round building company gold logo</td>\n",
       "      <td>0.537708</td>\n",
       "      <td>956.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>956.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000613640</td>\n",
       "      <td>62129823dacee6358a619a9ca25f0f9c069efa480a0dab...</td>\n",
       "      <td>https://cdn.lifestyleasia.com/wp-content/uploa...</td>\n",
       "      <td>a cozy living room with a Christmas tree and a...</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>10 easy, no humbug tips for that perfect Chris...</td>\n",
       "      <td>0.722248</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>3505.0</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>3505.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000621773</td>\n",
       "      <td>3620f54366401bd7739750d1e85b64ca41020497117ea7...</td>\n",
       "      <td>https://image.invaluable.com/housePhotos/galle...</td>\n",
       "      <td>a bird statue that is perched on top of a brow...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>Chinese Export Sterling Enameled Hawk on Base</td>\n",
       "      <td>0.555684</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591877</th>\n",
       "      <td>646166</td>\n",
       "      <td>004161450</td>\n",
       "      <td>37050df597ddbe1ad6f7e68ea49850af2b952dc34faaac...</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0106/5235/25...</td>\n",
       "      <td>a woman wearing a pink dress, standing outside...</td>\n",
       "      <td>0.986169</td>\n",
       "      <td>Bohemian printed beach skirt female hanging ne...</td>\n",
       "      <td>0.825665</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>{\"Image HostComputer\": \"imagery4\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591878</th>\n",
       "      <td>646167</td>\n",
       "      <td>004161435</td>\n",
       "      <td>f271089838b7a6695d50370a920893f088ac3e86b85018...</td>\n",
       "      <td>https://profile-images.xing.com/images/c645457...</td>\n",
       "      <td>a man wearing glasses and a green shirt. He is...</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>Felix roth gesch ftsf hrer kommunikationsdesig...</td>\n",
       "      <td>0.612309</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591879</th>\n",
       "      <td>646168</td>\n",
       "      <td>004161361</td>\n",
       "      <td>2fafded547cd8cf195be31836eba24c65a62fbc33021bd...</td>\n",
       "      <td>https://images.unsplash.com/photo-151232829114...</td>\n",
       "      <td>a man standing on a beach at sunset. He is wea...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>photography of man standing near water body</td>\n",
       "      <td>0.505804</td>\n",
       "      <td>6720.0</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>6720.0</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591880</th>\n",
       "      <td>646169</td>\n",
       "      <td>004161484</td>\n",
       "      <td>3d730cbb0cea2da7c63e1bab5a4b8c883561d78b83e1a3...</td>\n",
       "      <td>https://www.gannett-cdn.com/presto/2019/05/10/...</td>\n",
       "      <td>a large, colorful carousel in a carnival setti...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>Mall patrons walk past the 25-foot tall carous...</td>\n",
       "      <td>0.544864</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591881</th>\n",
       "      <td>646170</td>\n",
       "      <td>004161416</td>\n",
       "      <td>6920cfd192f49f3fc5d82cd393bc4aee8291366a6215eb...</td>\n",
       "      <td>https://szex-baba.hu/wp-content/uploads/2019/1...</td>\n",
       "      <td>a doll with a large chest, sitting on a chair....</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>JY Doll 170 cm H-Cup 20 Ã©lethÅ± szexbaba</td>\n",
       "      <td>0.621638</td>\n",
       "      <td>770.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591882 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index        key                                             sha256  \\\n",
       "0            0  000623232  d46d20ff9fa7212b3671b49ae73371672711e5265fff29...   \n",
       "1            1  000616883  dde6e39a297da2d7e8923bdf569c0e4704b29a69148b9d...   \n",
       "2            2  000620777  7196431abc8b8fabfa34f50be738ba1c0a4c72ccbc70ef...   \n",
       "3            3  000613640  62129823dacee6358a619a9ca25f0f9c069efa480a0dab...   \n",
       "4            4  000621773  3620f54366401bd7739750d1e85b64ca41020497117ea7...   \n",
       "...        ...        ...                                                ...   \n",
       "591877  646166  004161450  37050df597ddbe1ad6f7e68ea49850af2b952dc34faaac...   \n",
       "591878  646167  004161435  f271089838b7a6695d50370a920893f088ac3e86b85018...   \n",
       "591879  646168  004161361  2fafded547cd8cf195be31836eba24c65a62fbc33021bd...   \n",
       "591880  646169  004161484  3d730cbb0cea2da7c63e1bab5a4b8c883561d78b83e1a3...   \n",
       "591881  646170  004161416  6920cfd192f49f3fc5d82cd393bc4aee8291366a6215eb...   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://img1.etsystatic.com/000/0/5370909/il_f...   \n",
       "1       https://www.aktay.net/wp-content/uploads/2014/...   \n",
       "2       https://cdn5.vectorstock.com/i/1000x1000/03/14...   \n",
       "3       https://cdn.lifestyleasia.com/wp-content/uploa...   \n",
       "4       https://image.invaluable.com/housePhotos/galle...   \n",
       "...                                                   ...   \n",
       "591877  https://cdn.shopify.com/s/files/1/0106/5235/25...   \n",
       "591878  https://profile-images.xing.com/images/c645457...   \n",
       "591879  https://images.unsplash.com/photo-151232829114...   \n",
       "591880  https://www.gannett-cdn.com/presto/2019/05/10/...   \n",
       "591881  https://szex-baba.hu/wp-content/uploads/2019/1...   \n",
       "\n",
       "                                            llava_caption  nsfw_prediction  \\\n",
       "0       a black and white drawing of a man climbing a ...         0.000050   \n",
       "1       a magazine or brochure that is open to a page ...         0.000372   \n",
       "2       a gold and black building logo. The logo featu...         0.000272   \n",
       "3       a cozy living room with a Christmas tree and a...         0.000678   \n",
       "4       a bird statue that is perched on top of a brow...         0.000138   \n",
       "...                                                   ...              ...   \n",
       "591877  a woman wearing a pink dress, standing outside...         0.986169   \n",
       "591878  a man wearing glasses and a green shirt. He is...         0.000191   \n",
       "591879  a man standing on a beach at sunset. He is wea...         0.000034   \n",
       "591880  a large, colorful carousel in a carnival setti...         0.000040   \n",
       "591881  a doll with a large chest, sitting on a chair....         0.765568   \n",
       "\n",
       "                                                  alt_txt  alt_txt_similarity  \\\n",
       "0       Art Deco Man Cave : Rockwell kent art deco man...            0.592961   \n",
       "1                                pergo broÅŸÃ¼r tasarÄ±mÄ± 12            0.644895   \n",
       "2                        Round building company gold logo            0.537708   \n",
       "3       10 easy, no humbug tips for that perfect Chris...            0.722248   \n",
       "4           Chinese Export Sterling Enameled Hawk on Base            0.555684   \n",
       "...                                                   ...                 ...   \n",
       "591877  Bohemian printed beach skirt female hanging ne...            0.825665   \n",
       "591878  Felix roth gesch ftsf hrer kommunikationsdesig...            0.612309   \n",
       "591879        photography of man standing near water body            0.505804   \n",
       "591880  Mall patrons walk past the 25-foot tall carous...            0.544864   \n",
       "591881            JY Doll 170 cm H-Cup 20 Ã©lethÅ± szexbaba            0.621638   \n",
       "\n",
       "         width  height  original_width  original_height  \\\n",
       "0       1000.0  1013.0          1000.0           1013.0   \n",
       "1       1600.0  1143.0          1600.0           1143.0   \n",
       "2        956.0  1080.0           956.0           1080.0   \n",
       "3       5258.0  3505.0          5258.0           3505.0   \n",
       "4       1000.0  1237.0          1000.0           1237.0   \n",
       "...        ...     ...             ...              ...   \n",
       "591877   800.0  1000.0           800.0           1000.0   \n",
       "591878  1024.0  1024.0          1024.0           1024.0   \n",
       "591879  6720.0  4480.0          6720.0           4480.0   \n",
       "591880  2595.0  1921.0          2595.0           1921.0   \n",
       "591881   770.0  1155.0           770.0           1155.0   \n",
       "\n",
       "                                      exif  \n",
       "0                                       {}  \n",
       "1                                       {}  \n",
       "2                                       {}  \n",
       "3                                       {}  \n",
       "4                                       {}  \n",
       "...                                    ...  \n",
       "591877  {\"Image HostComputer\": \"imagery4\"}  \n",
       "591878                                  {}  \n",
       "591879                                  {}  \n",
       "591880                                  {}  \n",
       "591881                                  {}  \n",
       "\n",
       "[591882 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_dataset = pd.read_parquet('data/laion-pop/laion_pop.parquet')\n",
    "parquet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeba53a4-846e-4352-a153-ea023bf76ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_caption(parquet, key):\n",
    "    caption = parquet[parquet['key'] == key]['llava_caption']\n",
    "    caption = caption[caption.index[0]]\n",
    "    image = Image.open(f'data/images/{key}.jpg')\n",
    "    return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d504ee9b-93a9-4865-886b-37e0bfcc298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, cap = get_image_and_caption(parquet_dataset, '000620537')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f62a10-d906-4d9f-b020-169991592343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPTokenizer, CLIPTextModel, set_seed\n",
    "\n",
    "# model_id = \"data/model_bins/stable-diffusion-xl-base-1.0/tokenizer\"  # Replace with your model ID\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6f956e-ec41-4b72-a3f3-689fbe7b89a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4464"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path_to_pics = 'data/images/'\n",
    "all_pics_paths = glob.glob(path_to_pics + '*')\n",
    "len(all_pics_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e48e0f-966d-4967-acaa-d7002bef406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import crop\n",
    "\n",
    "class LaionHQ(Dataset):\n",
    "    def __init__(self, dataframe, path_to_pics):\n",
    "        self.dataframe = dataframe\n",
    "        self.path_to_pics = path_to_pics\n",
    "        all_pics_paths = glob.glob(path_to_pics + '*')\n",
    "        self.unique_imgs = [i.split('/')[-1].split('.')[-2] for i in all_pics_paths]\n",
    "        self.dataframe.index = self.dataframe.key\n",
    "        \n",
    "        self.train_resize = transforms.Resize(1024, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        self.train_crop = transforms.CenterCrop(1024) if False else transforms.RandomCrop(1024)\n",
    "        # self.train_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "        \n",
    "        self.train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key = self.unique_imgs[idx]\n",
    "        path_to_img = os.path.join(self.path_to_pics, key + '.jpg')\n",
    "        image = Image.open(path_to_img).convert('RGB')\n",
    "        original_size = (image.height, image.width)\n",
    "        \n",
    "        image = self.train_resize(image)\n",
    "        \n",
    "        y1, x1, h, w = self.train_crop.get_params(image, (1024, 1024))\n",
    "        image = crop(image, y1, x1, h, w)\n",
    "            \n",
    "        image = self.train_crop(image)\n",
    "        # image = self.train_flip(image)\n",
    "        pixel_values = self.train_transforms(image)\n",
    "        crop_top_left = (y1, x1)\n",
    "\n",
    "        return  pixel_values, original_size, crop_top_left, self.dataframe.loc[key].llava_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45eb0a76-22a7-41fe-b7d5-24a1010f87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# train_resize = transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "# train_crop = transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution)\n",
    "# train_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "# train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bb7a19-e7ed-42a3-95d0-e25e2bc46cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LaionHQ(parquet_dataset, 'data/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90790454-1d89-49d5-af06-d8e3e6ac0c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9059, -0.9137, -0.9137,  ..., -0.7098, -0.7098, -0.7098],\n",
       "          [-0.8980, -0.9059, -0.9059,  ..., -0.7098, -0.7098, -0.7020],\n",
       "          [-0.8824, -0.8902, -0.8980,  ..., -0.7098, -0.7098, -0.7020],\n",
       "          ...,\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7020, -0.7020, -0.6941],\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7098, -0.7098, -0.7098],\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7098, -0.7098, -0.7098]],\n",
       " \n",
       "         [[-0.9059, -0.9137, -0.9059,  ..., -0.6784, -0.6784, -0.6784],\n",
       "          [-0.8980, -0.9059, -0.8980,  ..., -0.6784, -0.6784, -0.6706],\n",
       "          [-0.8824, -0.8902, -0.8980,  ..., -0.6784, -0.6784, -0.6706],\n",
       "          ...,\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7412, -0.7412, -0.7333],\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7490, -0.7490, -0.7490],\n",
       "          [-0.9137, -0.9137, -0.9137,  ..., -0.7490, -0.7490, -0.7490]],\n",
       " \n",
       "         [[-0.9059, -0.9137, -0.9137,  ..., -0.6863, -0.6863, -0.6863],\n",
       "          [-0.8980, -0.9059, -0.9059,  ..., -0.6863, -0.6863, -0.6784],\n",
       "          [-0.8824, -0.8902, -0.8980,  ..., -0.6863, -0.6863, -0.6784],\n",
       "          ...,\n",
       "          [-0.8980, -0.8980, -0.9059,  ..., -0.7647, -0.7647, -0.7569],\n",
       "          [-0.8980, -0.8980, -0.9059,  ..., -0.7725, -0.7725, -0.7725],\n",
       "          [-0.8980, -0.8980, -0.9059,  ..., -0.7725, -0.7725, -0.7725]]]),\n",
       " (1024, 768),\n",
       " (146, 0),\n",
       " \"a portrait of Jimi Hendrix, a famous musician, wearing a red jacket and a purple scarf. He is wearing a hat, which is a distinctive feature of his style. The portrait is a painting, capturing the essence of the musician's iconic look. The background is dark, which adds to the focus on the subject and creates a dramatic effect. The image is a tribute to the legendary musician and his impact on the music industry.\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e0b0c2-6c07-4178-9d78-370e1398d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "set_seed(42)  # Replace with your seed\n",
    "\n",
    "\n",
    "model_path = \"data/model_bins/stable-diffusion-xl-base-1.0\"  # Replace with your model ID\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer_one = CLIPTokenizer.from_pretrained(os.path.join(model_path, 'tokenizer'))\n",
    "tokenizer_two = CLIPTokenizer.from_pretrained(os.path.join(model_path, 'tokenizer_2'))\n",
    "\n",
    "text_encoder_one = CLIPTextModel.from_pretrained(os.path.join(model_path, 'text_encoder'))\n",
    "text_encoder_two = CLIPTextModel.from_pretrained(os.path.join(model_path, 'text_encoder_2'))\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(os.path.join(model_path, 'vae')).to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(os.path.join(model_path, 'unet')).to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee44a1-a122-4a51-a15a-76018ee1f9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c25cc1-de64-416a-921f-c71118b76cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in unet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f56c96-9cf3-4d9f-bc9e-fed3eff08c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for param in unet.parameters():\n",
    "    if param.requires_grad:\n",
    "        total_params += param.numel()\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40270597-6d21-4b0a-b60f-29e0eec8bc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_lora_config = LoraConfig(\n",
    "    r=8, init_lora_weights=\"gaussian\", target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
    ")\n",
    "\n",
    "unet.add_adapter(unet_lora_config)\n",
    "\n",
    "# The text encoder comes from ðŸ¤— transformers, we will also attach adapters to it.\n",
    "\n",
    "    # ensure that dtype is float32, even if rest of the model that isn't trained is loaded in fp16\n",
    "# text_lora_config = LoraConfig(\n",
    "#     r=8, init_lora_weights=\"gaussian\", target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "# )\n",
    "# text_encoder.add_adapter(text_lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35233b6c-b3b8-4aa7-842d-a8f2e82aaf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder.add_adapter(text_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de88f961-ec6d-4c2a-85a3-56b7969fa601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params = 0\n",
    "# for param in text_encoder.parameters():\n",
    "#     if param.requires_grad:\n",
    "#         total_params += param.numel()\n",
    "# total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0b5f576-f471-4460-8c5c-6b9d4e2a7d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11612160"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = 0\n",
    "for param in unet.parameters():\n",
    "    if param.requires_grad:\n",
    "        total_params += param.numel()\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f983b94-1432-4abb-ba7d-a6a2b7a6bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_one = text_encoder_one.cuda()\n",
    "text_encoder_two = text_encoder_two.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a647b-277a-4f03-8e20-818911e3c811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ca666da-dd2a-4cb1-a1a8-feefd2d5ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_device(token_dict, device):\n",
    "    for key in token_dict:\n",
    "        token_dict[key] = token_dict[key].to(device)\n",
    "    return token_dict\n",
    "\n",
    "def tokenize_prompt(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    return text_input_ids\n",
    "\n",
    "def encode_prompt(text_encoders, tokenizers, prompt, text_input_ids_list=None):\n",
    "    prompt_embeds_list = []\n",
    "\n",
    "    for i, text_encoder in enumerate(text_encoders):\n",
    "        if tokenizers is not None:\n",
    "            tokenizer = tokenizers[i]\n",
    "            text_input_ids = tokenize_prompt(tokenizer, prompt)\n",
    "        else:\n",
    "            assert text_input_ids_list is not None\n",
    "            text_input_ids = text_input_ids_list[i]\n",
    "\n",
    "        prompt_embeds = text_encoder(\n",
    "            text_input_ids.to(text_encoder.device),\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)\n",
    "        prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
    "    pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)\n",
    "    return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "def compute_vae_encodings(images, vae):\n",
    "    pixel_values = torch.stack(list(images))\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    pixel_values = pixel_values.to(vae.device, dtype=vae.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "    model_input = model_input * vae.config.scaling_factor\n",
    "    return model_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fdda734-6083-4807-b3fb-ba093cae0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Return batch.keys() == model_input, original_sizes, crop_top_lefts, prompt_embeds, pooled_prompt_embeds\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    images, original_sizes, crop_top_lefts, prompts = zip(*batch)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    # model_input = compute_vae_encodings(images, vae)\n",
    "    # output.update(model_input)\n",
    "    output.update(\n",
    "        {\n",
    "            'img_tensors': images,\n",
    "            'prompts': prompts\n",
    "        }\n",
    "    )\n",
    "    output.update({\n",
    "        'original_sizes' : original_sizes,\n",
    "        'crop_top_lefts' : crop_top_lefts,\n",
    "    })\n",
    "    # output.update(encode_prompt(prompts, text_encoders, tokenizers))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f80bc44e-5136-4571-9559-61e726d05ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoders = [text_encoder_one, text_encoder_two]\n",
    "tokenizers = [tokenizer_one, tokenizer_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7e0bc15-5cdd-42f2-8fcd-22ee43444518",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, collate_fn = custom_collate_fn, num_workers = 8, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "53b76dd8-e568-4321-9901-5f28b333d7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_tensors': tensor([[[[ 0.0588,  0.1216,  0.1608,  ...,  0.3255,  0.4118,  0.4745],\n",
       "           [ 0.0745,  0.1137,  0.1529,  ...,  0.3255,  0.3804,  0.4431],\n",
       "           [ 0.0431,  0.0902,  0.1451,  ...,  0.3255,  0.3569,  0.3961],\n",
       "           ...,\n",
       "           [-0.7804, -0.7961, -0.8196,  ...,  0.4275,  0.4353,  0.4431],\n",
       "           [-0.7882, -0.7961, -0.8275,  ...,  0.4039,  0.4275,  0.4118],\n",
       "           [-0.7804, -0.7961, -0.8275,  ...,  0.4118,  0.4118,  0.4039]],\n",
       " \n",
       "          [[ 0.1137,  0.1765,  0.2235,  ...,  0.4118,  0.4196,  0.4275],\n",
       "           [ 0.1294,  0.1686,  0.2157,  ...,  0.4118,  0.4039,  0.4196],\n",
       "           [ 0.1059,  0.1529,  0.2235,  ...,  0.4118,  0.4039,  0.4118],\n",
       "           ...,\n",
       "           [-0.8902, -0.8980, -0.9216,  ...,  0.3020,  0.3176,  0.3176],\n",
       "           [-0.8745, -0.8824, -0.9137,  ...,  0.2941,  0.3176,  0.2941],\n",
       "           [-0.8588, -0.8745, -0.9137,  ...,  0.3098,  0.3020,  0.2863]],\n",
       " \n",
       "          [[ 0.0510,  0.1137,  0.1529,  ...,  0.2863,  0.2627,  0.2549],\n",
       "           [ 0.0667,  0.1059,  0.1451,  ...,  0.2863,  0.2549,  0.2471],\n",
       "           [ 0.0353,  0.0902,  0.1451,  ...,  0.2863,  0.2471,  0.2314],\n",
       "           ...,\n",
       "           [-0.9765, -0.9686, -0.9608,  ...,  0.2706,  0.2784,  0.2784],\n",
       "           [-0.9686, -0.9608, -0.9686,  ...,  0.2549,  0.2706,  0.2549],\n",
       "           [-0.9686, -0.9686, -0.9686,  ...,  0.2627,  0.2627,  0.2471]]]]),\n",
       " 'prompts': ('a young girl sleeping in a bed. She is laying down with her arms tucked under her chest, and her head is resting on a pillow. The bed is covered with a white comforter, and there are two pillows, one of which the girl is resting her head on. The scene is depicted in a painting style, with a soft and warm color palette. The girl appears to be peacefully asleep, conveying a sense of innocence and vulnerability. The overall atmosphere of the image is calm and serene.',),\n",
       " 'original_sizes': ((3274, 2500),),\n",
       " 'crop_top_lefts': ((177, 0),)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f857f4bd-c0f7-4dcf-b75a-ab1c3209f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_scheduler\n",
    "import math\n",
    "lr_warmup_steps = 500\n",
    "epochs = 1\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader))\n",
    "\n",
    "max_train_steps = num_update_steps_per_epoch * epochs\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'constant',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps * 1,\n",
    "    num_training_steps=max_train_steps * 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "262c3e92-0780-4964-b49e-c62fdf6cdc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74718dbd3bdd43b2899861e3e4ed1df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vae computed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100096 and 2816x1280)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m prompt_embeds, pooled_prompt_embeds \u001b[38;5;241m=\u001b[39m encode_prompt(text_encoders, tokenizers, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     38\u001b[0m unet_added_conditions\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: pooled_prompt_embeds})\n\u001b[0;32m---> 40\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet_added_conditions\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet computed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Get the target for loss depending on the prediction type\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:981\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m     add_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([text_embeds, time_embeds], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    980\u001b[0m     add_embeds \u001b[38;5;241m=\u001b[39m add_embeds\u001b[38;5;241m.\u001b[39mto(emb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 981\u001b[0m     aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;66;03m# Kandinsky 2.2 - style\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m added_cond_kwargs:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/embeddings.py:226\u001b[0m, in \u001b[0;36mTimestepEmbedding.forward\u001b[0;34m(self, sample, condition)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_proj(condition)\n\u001b[0;32m--> 226\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(sample)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100096 and 2816x1280)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "        model_input = compute_vae_encodings(batch['img_tensors'], vae)\n",
    "        print('vae computed')\n",
    "        noise = torch.randn_like(model_input)\n",
    "\n",
    "        bsz = model_input.shape[0]\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (bsz,), device=model_input.device\n",
    "        )\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the model input according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n",
    "\n",
    "        def compute_time_ids(original_size, crops_coords_top_left):\n",
    "            # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids\n",
    "            target_size = (1024, 1024)\n",
    "            add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "            add_time_ids = torch.tensor([add_time_ids])\n",
    "            add_time_ids = add_time_ids.to(unet.device, dtype=torch.float16)\n",
    "            return add_time_ids\n",
    "\n",
    "        add_time_ids = torch.cat(\n",
    "            [compute_time_ids(s, c) for s, c in zip(batch[\"original_sizes\"], batch[\"crop_top_lefts\"])]\n",
    "        ).to(unet.device)\n",
    "        \n",
    "\n",
    "        unet_added_conditions = {\"time_ids\": add_time_ids}\n",
    "        \n",
    "        prompt_embeds, pooled_prompt_embeds = encode_prompt(text_encoders, tokenizers, batch['prompts'])\n",
    "        \n",
    "        unet_added_conditions.update({\"text_embeds\": pooled_prompt_embeds})\n",
    "\n",
    "        model_pred = unet(\n",
    "            noisy_model_input, timesteps.to(unet.device), prompt_embeds, added_cond_kwargs=unet_added_conditions\n",
    "        ).sample\n",
    "        print('unet computed')\n",
    "        # Get the target for loss depending on the prediction type\n",
    "\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "        snr_gamma = None\n",
    "\n",
    "        if snr_gamma is None:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        else:\n",
    "            # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
    "            # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
    "            # This is discussed in Section 4.2 of the same paper.\n",
    "            snr = compute_snr(noise_scheduler, timesteps)\n",
    "            if noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                # Velocity objective requires that we add one to SNR values before we divide by them.\n",
    "                snr = snr + 1\n",
    "            mse_loss_weights = (\n",
    "                torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr\n",
    "            )\n",
    "\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # Gather the losses across all processes for logging (if we use distributed training).\n",
    "        # avg_loss = accelerator.gather(loss.repeat(2)).mean()\n",
    "\n",
    "        train_loss += loss\n",
    "        print(loss.item())\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # if accelerator.sync_gradients:\n",
    "        #     accelerator.clip_grad_norm_(params_to_optimize, args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1034e52-7b00-4a15-a88f-48176d779a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_prompt_embeds.size()[1] / 2816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c5f2e92-b454-4656-98c1-9c85021aaaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1200.,  961.,  167.,    0., 1024., 1024.]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_time_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "646c66d0-d0bc-461b-a3d2-ef702695e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 2048])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "953da336-8bd4-4614-89ea-a943150b5569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.time_proj.num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfc6107a-cdcc-4206-8fb7-6052b6af8eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98560])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_added_conditions['text_embeds'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9622bc-0e00-41c2-ac67-9a6b039c7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_model_input, timesteps, prompt_embeds, added_cond_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef61100a-0f64-4089-b90d-7a7466e2b160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embeds.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093313e-721a-4c0a-9566-098dc6a463a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(unet, dataloader, epochs=3)  # Adjust epochs as needed\n",
    "\n",
    "# Step 5: Validation and Inference\n",
    "# Define a function for validation or inference\n",
    "def generate_images(model, prompts, num_images=1):\n",
    "    model.eval()\n",
    "    generated_images = []\n",
    "    for prompt in prompts:\n",
    "        # Your inference logic here\n",
    "        pass\n",
    "    return generated_images\n",
    "\n",
    "# Generate images\n",
    "sample_prompts = [\"A futuristic city\", \"A landscape painting\"]  # Replace with your prompts\n",
    "generated_images = generate_images(unet, sample_prompts)\n",
    "\n",
    "# Display or save the generated images\n",
    "for img in generated_images:\n",
    "    display(img)  # or save them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python-pytorch2_0]",
   "language": "python",
   "name": "conda-env-python-pytorch2_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
